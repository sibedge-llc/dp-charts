#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: kyuubi-defaults
  labels:
    app: {{ template "kyuubi.name" . }}
data:
  kyuubi-defaults.conf: |
    # Kyuubi configurations https://kyuubi.apache.org/docs/latest/deployment/settings.html
    kyuubi.authentication=NONE
    kyuubi.engine.share.level=USER
    kyuubi.frontend.bind.host={{ .Values.server.bind.host }}
    kyuubi.frontend.bind.port={{ .Values.server.bind.port }}
    # kyuubi.metrics.reporters=PROMETHEUS

    # Zookeeper required for kyuubi HA
    kyuubi.ha.zookeeper.quorum=zookeeper.zookeeper.svc.cluster.local:2181

    # Required for k8s CLUSTER mode
    kyuubi.engine.connection.url.use.hostname=false
    
    # Spark configurations
    spark.master=k8s://https://kubernetes.default.svc.cluster.local
    spark.submit.deployMode=cluster
    spark.kubernetes.container.image=skhakhulin/base_spark:1.0.0 

    spark.kubernetes.namespace=kyuubi
    spark.kubernetes.authenticate.serviceAccountName=kyuubi-kyuubi
    spark.kubernetes.authenticate.driver.serviceAccountName=kyuubi-kyuubi

    # Cert & token required to connect to k8s api
    # spark.kubernetes.authenticate.caCertFile=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    # spark.kubernetes.authenticate.oauthTokenFile=/var/run/secrets/kubernetes.io/serviceaccount/token

    # Required for k8s CLUSTER mode, to transfer kyuubi sqlengine jar to driver pod
    spark.kubernetes.file.upload.path=s3a://spark/kyuubi/ 
    
    # S3     # НЕ УКАЗЫВАТЬ spark.hadoop.fs.s3a.endpoint !
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.fast.upload=true
    spark.hadoop.fs.s3a.path.style.access=true
    # spark.hadoop.fs.s3a.connection.ssl.enabled=false
    spark.hadoop.fs.s3a.endpoint=ewr1.vultrobjects.com
    spark.hadoop.fs.s3a.access.key=6DKBB4FU8HIDEJD49GCF
    spark.hadoop.fs.s3a.secret.key=QtELTrZUy402hQ5wCUowfUaiVpgWS821FaU4iFP6
    spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp
    
    # Logs
    # spark.eventLog.dir=s3a://spark/kyuubi/logs/
    # spark.eventLog.enabled=true
    # spark.eventLog.rotation.enabled=true
    # spark.eventLog.rotation.interval=3600
    # spark.eventLog.rotation.minFileSize=100m
    # spark.eventLog.rotation.maxFilesToRetain=2
    
    # Spark SQL
    spark.sql.adaptive.enabled=true
    spark.sql.broadcastTimeout=30000
    spark.sql.parquet.datetimeRebaseModeInWrite=LEGACY
    spark.sql.parquet.mergeSchema=true
    spark.sql.sources.partitionOverwriteMode=dynamic
    spark.sql.warehouse.dir=s3a://spark/warehouse/
    
    spark.sql.catalogImplementation=hive
    spark.sql.hive.metastore.sharedPrefixes=org.postgresql
    
    spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:postgresql://postgresql.postgresql.svc.cluster.local:5432/metastore?createDatabaseIfNotExist=true
    spark.hadoop.javax.jdo.option.ConnectionPassword=password
    spark.hadoop.javax.jdo.option.ConnectionUserName=hive
    spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
    
    spark.hadoop.datanucleus.schema.autoCreateTables=true

    spark.hadoop.hive.metastore.schema.verification=false
    spark.hadoop.hive.metastore.schema.verification.record.version=false
    
    # Metrics
    # spark.ui.prometheus.enabled=true
    # spark.kubernetes.driver.annotation.prometheus.io/scrape=true
    # spark.kubernetes.driver.annotation.prometheus.io/path=/metrics/executors/prometheus/
    # spark.kubernetes.driver.annotation.prometheus.io/port=4040
    # spark.executor.processTreeMetrics.enabled=true
    
    # hive-site.xml configmap for metastore
    # spark.kubernetes.hadoop.configMapName={{ .Release.Name }}-kyuubi-spark-configmap
    
    # path to spark pod template
    # spark.kubernetes.driver.podTemplateFile=/opt/spark/podtemplate/pod_template.yml
    # spark.kubernetes.executor.podTemplateFile=/opt/spark/podtemplate/pod_template.yml
    # storing pod template in s3 does not work
    
    #####spark.kubernetes.driver.podTemplateFile=s3a://uchiru-bi-dwh-backup/kyuubi/pod_template.yml
    #####spark.kubernetes.executor.podTemplateFile=s3a://uchiru-bi-dwh-backup/kyuubi/pod_template.yml

#  log4j.properties: |
#    # всё в консоль
#    log4j.rootCategory=INFO, console
#    log4j.appender.console=org.apache.log4j.ConsoleAppender
#    log4j.appender.console.target=System.err
#    log4j.appender.console.layout=org.apache.log4j.PatternLayout
#    log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss.SSS} %p %c{2}: %m%n
#    # отключить в логе ошибки 'org.apache.thrift.transport.TSaslTransportException: No data or no sasl data in the stream' из-за балансера:
#    # пока тоже не работает, нужно добавлять джарники log4j и подбирать
#    log4j.appender.console.filter.1=org.apache.log4j.filter.RegexFilter
#    log4j.appender.console.filter.1.Regex=org.apache.thrift.transport.TSaslTransportException
#    log4j.appender.console.filter.1.onMatch=DENY
#    # найдено в гугле, не работает с коробочной версией log4j
#    #log4j.appender.console.filter.1=org.apache.log4j.filter.ExpressionFilter
#    #log4j.appender.console.filter.1.Expression=EXCEPTION ~= org.apache.thrift.transport.TSaslTransportException
#    #log4j.appender.console.filter.1.AcceptOnMatch=false